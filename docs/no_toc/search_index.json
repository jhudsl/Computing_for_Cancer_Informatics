[["index.html", "Computing for Cancer Informatics About This Course", " Computing for Cancer Informatics 2022-03-04 About This Course This course is part of a series of courses for the Informatics Technology for Cancer Research (ITCR) called the Informatics Technology for Cancer Research Education Resource. This material was created by the ITCR Training Network (ITN) which is a collaborative effort of researchers around the United States to support cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at www.itcrtraining.org for more information. Except where otherwise indicated, the contents of this course are available for use under the Creative Commons Attribution 4.0 license. You are free to adapt and share the work, but you must give appropriate credit, provide a link to the license, and indicate if changes were made. Sample attribution: Data Management for Cancer Research by Johns Hopkins Data Science Lab (CC-BY 4.0). You can download the illustrations by clicking here. "],["introduction.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Target Audience 1.3 Curriculum", " Chapter 1 Introduction 1.1 Motivation One of the key cancer informatics challenges is dealing with and managing the explosion of large data from multiple sources that are often too large to work with on typical personal computers. This course is designed to help researchers and investigators to understand the basics of computing and to familiarize them with various computing options to ultimately help guide their decisions on the topic. 1.2 Target Audience This course is intended for researchers (including postdocs and students) with limited to intermediate experience with informatics research. The conceptual material will also be useful for those in management roles who are collecting data and using informatics pipelines. 1.3 Curriculum The course will cover the key underlying principles and concepts in computing. It will cover concrete discussions of the differences between cloud and local computing. The course will highlight a number of computing options and etiquette for using shared resources. "],["basic-building-block-of-computers.html", "Chapter 2 Basic Building Block of Computers 2.1 Transistors 2.2 ALU - Arithmetic Logic Unit 2.3 Conclusion", " Chapter 2 Basic Building Block of Computers First we would like to start off with some background information about how computers actually work (in this chapter and the next). We feel that this information, will be very helpful for understanding what computing resources your research will actually require. This information will also better enable you to discuss your computing needs with computing experts, for example people who manage shared computing resources that you might want to use. If you are already more familiar with these topics, we hope that the next two chapters might fill in possible knowledge gaps, point you to more resources, or at least provide some entertaining information regarding the history and future of computers that might change your perspective. 2.1 Transistors Luckily, you are likely not going to need to become a bee keeper to perform your computational research (unless of course that interests you)! Instead, computers rely on millions to billions of transistors (“Transistor Count” 2021). Transistors are one of if not the most important basic building blocks of computers. There are many different types of transistors, but they often look like a rectangle with three prongs (“How Does a Transistor Work?” n.d.). Transistors behave like electronic switches or gates that either allow or do not allow current to flow through a particular part of a circuit (V.Ryan 2002). [Source] Inside the plastic, is often silicon, or some other semiconductive crystal. Semiconductors materials are needed because they way that they conduct electricity can be modified by the application of more electricity, making them the perfect option for creating an electrical switch. Silicon is especially useful, because it doesn’t cause the circuit to get very hot, unlike previously used materials. It is also very abundant, in fact, it is the second most common element of the Earth’s crust! (“Silicon - Wikipedia” n.d.). If the transistor receives a small amount of current to one of the prongs (called the base), this turns it on, and allows the larger current for the circuit to pass through the transistor (from a prong called the collector to the prong called the emitter). If the base prong of the transistor does not receive a small current than the transistor is off and the current for the circuit is not allowed to flow through the transistor. You may see how this is similar to ligand-gated ion or ionotropic channels in biological systems, where a channel remains closed and does not allow ions to pass through the membrane unless a ligand (like a neurotransmitter) binds to the channel protein, allowing the channel to open (“Ligand-Gated Ion Channel” 2021; “Ligand-Gated Ion Channels” 2011) and allowing ions to flow through the channel. The two states for the flow of current ultimately allow for the storage and use of binary data, which we think of a zeros and ones, but it is really the absence or presence of current with a voltage beyond a threshold for this part of the circuit. Thus the physical components of a computer are ultimately based on the assessment of only two states of current (0 (or FALSE) = below a threshold and 1 (or TRUE) = above a threshold), which is much easier to create than if it we needed to assess more nuanced levels of current. It turns out that this binary encoding of current as digital data is the basis for all the complex tasks that we use computers for everyday. Very importantly transistors have gotten much smaller over time. The smaller size of transistors has allowed for many more transistors to be used inside computers. Check out “Transistor Count” (2021) for more information about how the number of transistors in computers has grown over time. Early computers had thousands of transistors; now some supercomputers have trillions (“Transistor Count” 2021)! [Source] Both the smaller size of the transistors and the increased number of transistors have in part allowed computers to become faster and more powerful (Pokropivny et al. 2007). Thus transistors are a key reason why we have seen such an explosion of computing power and storage, which has facilitated what we have seen in the incredible expansion of data. These silicon transistors became so important for the field of electronics, that the time period of heavy computing development during the late 20th century and early 21st century is sometimes called the “Silicon Age”. This is also why many places in the world where there are many technological institutes are often called a name with the word “silicon”, such as Silicon Valley (“Silicon Valley” 2021). Here is an interesting article about what our next age might be about, and it has to do with changing the way we harness electrons (the current role of transistors) (Tom Ward 2017) — that’s how important they are! If you would like to learn more about the history of transistors and how they work check out this website (Woodford 2007). Finally, it is important to note that modern transistors have a 3D structure that allows them to be faster, more efficient, and more densely packed because now circuits can be layered in 3D structures, thus allowing for even more transistors to be included within modern computers (“FinFET” 2021). 2.2 ALU - Arithmetic Logic Unit The ALU is responsible for performing simple operations by using networks (in this case commonly called a circuit) of transistors. These simple operations include logical operations (AND, OR, NOT, etc.), and arithmetic operations (addition, subtraction, division, multiplication, etc.). Ultimately most of what we do everyday on our computers comes down to these simple operations. These operations are based on what is called Boolean logic or Boolean algebra, which was invented by George Boole in 1854 and largely comes down to thinking of possible sets of data (“How Do Logic Gates Work?” n.d.; “Boolean Algebra” 2021). For example, if we have two transistors, they could both be on, they could both be off, or one or the other could be on. Considering these possibilities, we can make overall descriptions about the flow of current to perform logical operations. Let’s take a moment to understand how networks of transistors work for AND and OR operations. We call a network for a logical operation a logic gate (“Logic Gate” 2021). Note that this is a simple illustration and in actual electronics, additional transistors are often used for greater sustainability, consistency, efficiency, and speed, largely based on controlling the level of current and voltage in more nuanced ways. In this illustration of the transistor AND gate, there are two transistors where the current going to the collector for each transistor is in series. This means the transistors are sequentially placed one after the other, where one receives current first before the other. A resulting high current output only occurs when both of the transistors allow for the flow of current. If either transistor is off or both of the transistors are off, then the current is not allowed to flow through, and the resulting digital output is zero (“AND Gate” 2021). In our next simple illustration, the transistor OR gate has two transistors in parallel, meaning they are next to one another each receiving the flow of current at the same time. A resulting high current output can occur when either of the transistors allows for the flow of current (“OR Gate” 2021). Importantly, using more complex arrangements of these logic gates can allow the computer to perform the arithmetic operations (“Adder (Electronics)” 2021). See here and here for more information on how this works. For example, to calculate the sum of 1 plus 1 we would need what is called a “full adder” which can be made a few ways but generally contains several logic gates that check the current status of the digits of the values being added. For example, part of an adder might check using logic if the first digit of the first number is the same as that of the second number to be summed with the first. Recall that each of these gates would be made up of multiple transistors, thus many transistors would be required even for adding 1 and 1. For larger numbers we would need more full adders (“Adder (Electronics)” 2022; “Binary Adder and Binary Addition Using Ex-OR Gates” n.d.). Thus You can see how the need for many transistors adds up quickly! To better understand how this is possible, we need to first talk about how computers use the two binary states of current to represent numeric values. In other words, we need to talk about binary data. If you would like to learn more about these logic gates with circuit diagrams, check out this website and this website for some visualizations. This website and this website also go into great detail. In case you are wondering about the semantics of phrases like the “flow of current”, check this discussion. 2.2.1 Binary data An ALU performs arithmetic operations using values represented in binary digits called bits (0 or 1) (recall that this is based on a state of current). Data like this is also called Boolean, based on George Boole system of algebra. These values do not mean their typical meanings from what we know numerically, but instead follow arithmetic rules using 2 as the base, as opposed to 10 which we are familiar with for our decimal system. What does this mean? With our decimal system when we reach a value of 10, we start to carry over the 1. With the binary system when we reach a value of 2, we start to carry over the 1 (“Boolean Algebra” 2021). Here we can see how the first 9 digits of the decimal system are represented in the binary system. Now taking this a step deeper, we can see how addition works with binary data. See here to learn more about binary calculations (“Binary Calculator” n.d.). This optional videoexplains further how transistors are used to add numbers together. 2.2.2 Flip-flops and registers Flip-flops are used for storing one bit of digital binary data. They are made of transistors (that’s right it’s transistors again!) and capacitors in a configuration that allows for the flip-flop to hold one of two states, thus enabling the storage of binary data (“Flip-Flop (Electronics)” 2021; “Memory Cell (Computing)” 2021). A group of flip-flops is called a register (“Hardware Register” 2021). You may have heard about a computer having a 64- or 32- bit operating system (more on this soon). These computers have registers with 64 bits or 32 bits respectively. Thus there are 64 flip-flops within the registers of a 64-bit system (“What Is 64-Bit (Wow64 and X64)?” n.d.). Each of these are capable of storing and processing binary values 64 digits in length (which works out to an unsigned integer in our decimal system of up to 2^64-1, or 18,446,744,073,709,551,615)(“What Is 64-Bit (Wow64 and X64)?” n.d.)! You may also be wondering how letters and other symbols are stored in this binary system. Letters are each assigned a numeric decimal value according to an encoding system such as the ASCII system, and these are converted into the binary form of this numeric number. Below you can see the decimal value for some of the symbols and letters: In the ASCII system, this ultimately works out to letters being stored by a standard 8 binary digits (or bits) (“ASCII” 2021). A group of 8 bits (8 digits of zeros and or ones) is called a byte (“What Is ASCII (American Standard Code for Information Interexchange)?” n.d.). Since this is consistent, this works well with computers that have registers that can store in sets of 8 bits. In fact, that is indeed how most computers work today. The “64-bit” part of what is called a 64-bit computer indicates what is called the word size or word length, which is the maximum unit of data that the computer can work with at a time (“Word (Computer Architecture)” 2021). This means that it can process binary numbers of up to 64 digits in length. Since 64 divided by 8 is 8, this means for a 64-bit computer, each register could store up to 64 bits or binary digits and thus can store 8 binary bytes. Note that it is possible to combine registers to make computations with larger numbers. Since each letters or symbol is encode by a byte (8 bits), this means up to 8 letters or symbols can be stored by a single register at a time. Other computers may work with a 32-bit word size, meaning that the registers can accommodate only 4 bytes at a time or 32 binary digits. As you might guess 64-bit computers are more capable of faster speeds and greater precision (by giving more decimal places) when computing operations with values that are larger than 32 binary digits, as compared to such operations using a 32-bit computer. Note that ASCII has largely been replaced since 1991 for Unicode, which allows for more characters, supporting languages like Chinese that require far more characters than the 256 that ASCII could support (“Unicode” 2021). However Unicode works in a similar way(“Unicode” 2021). Keep in mind that ALUs can only work with binary data. All different types of data like letters, words, numbers, code, etc. ultimately get encoded as 0s and 1s first for the computer to work with and after the computations are made, the computer then translates the data back to numeric and alphabetic form for us to understand. Thus computers do a lot of data conversions! Here’s a great video that puts everything we have explained so far together: Again, if you want to watch another video, this optional video that we told you about earlier explains how transistors are used to add numbers together. In this video you will see that many transistors (and several logic gates) are needed to even do a simple calculation of 1 + 1. You also learn that more complicated summations just require more transistors arranged in a similar manner. 2.3 Conclusion We hope that this chapter has given you some more knowledge about how computers are physically made. In conclusion, here are some of the major take-home messages: Computers rely on millions to billions of tiny transistors Transistors act like electrical switches that allow for the storage and processing of digital binary data Binary data is essentially the encoding of current states in the hardware of a computer as zeros and ones As transistors got smaller and more transistors were included in computers, computers got faster and more powerful (also due to other additional reasons) References "],["binary-data-to-computations.html", "Chapter 3 Binary Data to Computations 3.1 Conclusion", " Chapter 3 Binary Data to Computations Now that we are familiar with transistors and binary data, we will next discuss how computers process and store data. 3.0.1 CPU - Central Processing Unit The CPU is often called the brain of the computer. It has some confusing additional names, because it is such an important and prominent part of the computer, as it performs and orchestrates computational tasks “Central Processing Unit” (2021). It is sometimes called a processor or microprocessor (but technically these terms include both the CPU and other elements). The CPU is often what people are referring to when they describe a “computer chip” (which again technically includes other elements) “Central Processing Unit” (2021). The CPU is made up of several components, a few that are particularly important (two of which we have discussed): Arithmetic Logic Unit (ALU) Registers Control Unit (CU) A group of these components together is called a core. Multiple cores together are also referred to as CPUs. As you can see describing this can get kinda tricky. The component that we haven’t yet discussed, the Control Unit, coordinates the ALU and the data stored in the registers, so that the ALU can perform the operations on the right data stored in the registers at the right time (Bräunl 2008). Modern computers now have multiple cores. What does this mean? This means that there are multiple groups of the above components that can each process data within the same computer. A dual core CPU is a chip with two cores. A quad-core CPU is a chip with 4 cores and so on. This allows modern computers to perform multiple tasks at the same time instead of sequentially, such as 4 tasks simultaneously on a current typical laptop (with 4 cores). This makes our computers much faster than they used to be (“Central Processing Unit” 2021). In addition to the main CPU or CPUs or cores (chose your favorite name), computers may be equipped with specialized processors called GPUs which stands for graphics processing units that are especially efficient at tasks involving images (gaming et al. n.d.). Thus often tasks that involve images are done using the GPU(s) and not the CPU(s). This frees up the CPU(s) to continue on the tasks not involving images more efficiently. Note however, that GPU processors are also “generally programmable” (meaning they can work with different types of data) and can also be used to perform tasks that don’t involve images (gaming et al. n.d.). It’s also really good at doing something called parallel processing, which means dividing up a single task into multiple pieces that can be run simultaneously and thus allowing for running a task more efficiently overall. People also use GPU graphics cards which can add additional GPUs for more computational power (gaming et al. n.d.). Hyper-threading is also an option for improving processing. This technology started in 2002 by Intel (“Hyper-Threading - Wikipedia” n.d.). The idea is that while part of the same core is idle or waiting for a given task, another part of the same core can work to perform another task. This isn’t as efficient as a having another core or CPU, but it does improve efficiency (“What Is Hyper-threading HP® Tech Takes” n.d.; “Hyper-Threading - Wikipedia” n.d.). So many modern computer chips actually use all three efficiency boosters (having multiple cores, having GPUs, and using hyper-threading). Thus a chip with 4 cores that also has hyper-threading can work on 8 tasks simultaneously. Since it is now much easier to produce chips with multiple cores and because there are some security concerns with hyper-threading, the field seems to be moving away from hyper-threading [(“What Is Hyper-threading HP® Tech Takes” n.d.; “Hyper-Threading - Wikipedia” n.d.). 3.0.2 Memory or RAM - short-term memory OK, so we have already talked about how data can be stored in the registers within the CPU. This data or memory is used directly by the CPU during operations or tasks. However, our CPUs need additional quick access to instructional data to tell the CPU what to do to perform the operations and what data to use. This is also the data in a file that we are working with at a particular moment in time (“What Is RAM (Random-Access Memory)?” n.d.). This bring us to RAM, which stands for Random Access Memory. It is often simply referred to as memory. Ram is similarly made out of transistors and capacitors like the registers within the CPU, but it is located nearby but outside of the CPU (“What Is RAM (Random-Access Memory)?” n.d.; “How RAM Works” 2000). This type of memory is that it is temporary. Data is stored in RAM for only a short time, while your computer is running a task on it, but then it disappears. Due to the fact that what is stored disappears, this type of memory is also called volatile. This is why when you are working on a file, but forget to save it, you might lose your work (“What Is RAM (Random-Access Memory)?” n.d.; “How RAM Works” 2000). For more information about how RAM works, check out this website (“How RAM Works” 2000). 3.0.3 Storage - long-term memory We can also store data that we aren’t directly using when our computer is performing operations. So for example, our excel files and word files that aren’t currently in use. This type of memory is called storage and is sometimes referred to as long-term or non-volatile memory because electricity is not required to preserve this data. This type of memory is stored using hard disk drives (HDDs) also called hard drives or more recently solid-state drives (SSDs). The reason accessing this memory is slower than accessing data stored in RAM is that it is located further away from the CPU and data needs to be transferred from the storage to the CPU along a wire when a user wants to perform operations on such data. In addition the right data needs to be found out of all of your files, which also takes some time. Furthermore, the way in which data is retrieved from HDDs and SSDs is slower than that of RAM. This type of storage allows for much larger data capacity than RAM and it is also cheaper (“What Is a Hard Drive?” n.d.; “How a Hard Drive Works - ExtremeTech” n.d.). Hard disk drives store memory using magnetic methods (“How a Hard Drive Works - ExtremeTech” n.d.), while solid-state drives store memory using chips that have guess what?? They are made of yet again the important basic building block of computers - tiny bees! Oops, I mean transistors yet again, just like the CPU chip! See, those transistors are really important. SSDs allow for much faster reading and writing of files, as well as increased reliability. However, they are more expensive and they also wear out eventually (“What Is SSD (Solid-State Drive)?” n.d.). Here’s a great explanation for how HDDs work and the difference with SSDs. It will also introduce the concept of caching, which allows for faster use of data from storage for the CPU. It is a special kind of memory that’s even faster and closer to the CPU than RAM (“CPU Cache” 2021): See this link for more information about how SSDs work, and see here for an in depth explanation. 3.0.4 Hardware and software So far we have talked about the hardware of a computer, which is the physical components of a computer, while software is the code that tells the hardware how to function (“Computer Hardware” 2021; “Software” 2021). Software is also important to know about. Most importantly it is useful to know about operating systems. 3.0.5 Operating systems The operating system (sometimes simply called the OS) is a set of code or software that translates user interactions with the computer to tell the hardware (including memory and the CPU) of the computer what tasks the user wants the computer to perform and when (“Operating System” 2021). You can think of this as the basic code to keep the computer running and functional and to allow the user to use other forms of software, such as applications (“Operating System” 2021). Applications are specialized software programs like Microsoft Word, or an internet browser like Chrome that allow a user to do specific tasks on the computer. So your OS is what allows you to name, rename, move and save files. It helps you to keep track of memory and decides what memory should be used when and to run all of your application software. It also allows you to talk to other devices like printers or other computers. Examples of commonly used operating systems on computers and phones are: Microsoft Windows (such as Windows 10, Windows 11 etc.) macOS (notice the OS here - it might make more sense now why it is called this) Unix Linux Android Recall that we previously talked about how computers today are often called 64-bit? Operating systems are also designed in this way. A 64-bit operating system expects the hardware of the computer to allow for processing at least 64 bits of data at a time (the word size) (“Word (Computer Architecture)” 2021). If we have registers of at least this length in the CPU, than we can in fact perform operations on data that may be up to 64 bits in length. This also means that we can perform operations on values that take up less than 64 bits. This can be important because if you try to use an operating system that expects a longer word size than the hardware can accommodate, for example a 64-bit operating system on a 32-bit computer, this will not work. Application programs are also designed according to different word sizes and again you need to choose options that are equal to or less than the word size that your CPU can accommodate (“What Is 64-Bit (Wow64 and X64)?” n.d.). 3.0.6 Historical context Previously, back when a university might have one single computer, as they were so large and expensive (they didn’t use those nifty small transistors of today), computers didn’t have sophisticated operating systems and only one task could be performed at a time by one person at a time. Back then, tasks were just manually started, prioritized, and scheduled by humans. Tasks or programs (including sometimes data) could be printed or punched on cards (called punchcards, punch cards or punched cards) that would be loaded into the machine. Data and code would be manually indicated by punching or creating a hole in the card in certain locations. For example, columns might indicate different numeric or alphabetical values. It could really be a pain for users if they accidentally dropped the cards for the program they wanted to run, as you can imagine (“Punched Card” 2021)! There were many different kinds of punch cards over time, see Scott (2016) for a collection. The first operating system just allowed different programs to be run sequentially without someone manually starting each one. Now our personal computers can perform multiple tasks at the same time and schedule future tasks that our automatically run. Check out this video if you want to learn more about how these punch cards worked. See Tj (2017) for more information about operating systems and “Punched Card” (2021) for really interesting information about the history of punched cards. Also check out “History of Computing Hardware” (2021) for really interesting and more extensive history about how computer hardware was developed. Also, here is some fascinating additional reading on the role of women as computer operators starting in the 1940s. Initially computer science was actually thought of as a field for women, however this changed over time (and now women and gender minorities are hopefully becoming more represented) : Article titled: Woman pioneered computer programming. Then men took their industry over (Visions 2017) Article titled: Untold History of AI: Invisible Women Programmed America’s First Electronic Computer The “human computers” who operated ENIAC have received little credit (Schwartz 2019) 3.1 Conclusion We hope that this chapter has given you some more knowledge about how computers actually function. In conclusion, here are some of the major take-home messages: The central processing unit or CPU contains the Arithmetic Logic Unit or ALU which performs operations on data using transistor logic gates A CPU chip can contain multiple cores (also called CPUs) allowing a computer to perform multiple operational tasks at a time RAM is the memory for a computer for the tasks that its currently working on and is very fast to access because it is close to the CPU Storage on a hard drive or solid state drive is the memory for a computer that is long-term, such as files that you aren’t currently working on. It takes longer to access data from this memory as it has to travel to the CPU The operating system is what tells the computer what the user wants the computer to do and when Now that we know how a computer works in general, we will next discuss computing capacity, especially for informatics research, and how servers and cloud computing can help. References "],["computing-resources.html", "Chapter 4 Computing Resources 4.1 Data Sizes 4.2 Computing Capacity 4.3 File Sizes 4.4 Computing Options 4.5 Conclusion", " Chapter 4 Computing Resources In this chapter we will describe the basics about data size and computing capacity. We will discuss the computing and storage requirements for many types of cancer related data, as well as options to perform informatics work that might require more intensive computing capacity than your personal computer. 4.1 Data Sizes Recall that the smallest unit of data is a bit which is either a zero or a one. A group of 8 bits is called a byte, and most computers and phones, and software programs are constructed or designed in a way to accommodate groups of bytes at a time. For example a 32-bit machine can work with 4 bytes at a time and a 64-bit can work with 8 bytes at a time. But how big is a file that is 2 GB? When we sequence a genome, how large is that in terms of binary data? Can our local computer work with the size of data that we would like to work with? First let’s take a look at how the size of binary data is typically described and what this actually means in terms of bits and bytes: Now that we know how to describe binary data sizes, let’s next think about how much computing capacity typical computers have today. 4.2 Computing Capacity We have discussed a bit about CPUs and how they can help us perform more than one task at a time, but how many tasks can the CPU of an average computer do simultaneously these days? How much memory and storage do they typically have? What size of files can a typical computer handle? This information is sometimes called the specs of a computer. These values will probably change very soon, and different computers vary widely, but currently: Laptops can often perform 4-8 CPU tasks at once, and typically range from 4-16 GB in memory and 250 GB-1 TB of storage. This means that typical laptops can multitask quite well, have in some cases 16 gigabytes for random access memory to allow the CPU to work on relatively large tasks (as we can see from the previous table that GB are actually pretty large when you think about it), and possibly 1TB for the hard drive (and or SSD), meaning that you can store thousands of photos and files like PDFs, word documents etc. It turns out that you can store around 30,000 average size photos with 250GB, so a 1TB laptop can store quite a bit of data. Therefore overall, typical laptops today are pretty powerful compared to previous computers. Note that some programs require 16 or even 32 GB of memory to run. Desktops can perform and store similarly and sometimes to a degree better than a laptop for a similar price. Since less work needs to be done to make the desktop small and portable, sometimes you can get better storage and performance for the same price as a laptop. However, desktops often have better graphics processing capacity and displays and that might make up for the price difference (Antonio Villas-Boas 2019). This might be important to consider if you are going to need to visually inspect many images. Another benefit is that you can also sometimes find desktops with larger memory and storage options right off the shelf than typical laptops. It is also generally easier to add more memory to a desktop than it is to add to a laptop (Antonio Villas-Boas 2019). However of course, desktops certainly aren’t super portable! Some phones can compete with laptops by performing 6 CPU tasks at once and storing 6 GB in memory and 250 GB of storage. Check out this link to compare the prices of different macs and this link to compare specs for PC computers from HP. If you want to get really in-depth comparisons for PC or windows machines, check out this link (“UserBenchmark: Core I7-11700k Build Comparisons” n.d.). 4.2.1 Checking your computer capacity - Mac So what about your computer? How do you know how many cores it has or how much memory and storage it has? If you have a Mac, you can click on the apple symbol on the far left of your screen. Then click on the “About This Mac” button. You might see something like this: First we see the operating system is called Mojave. Next we see that the processor (which we now know is the CPU) is a 2.6 GigaHertz (GHz) Intel Core i7 chip. This means that the processor or CPU can process 2,600,000,000 operations in a second (this is called a clock cycle) (“What Is a Clock Cycle? - Definition from Techopedia” n.d.). That’s a lot compared to older computers which had clock cycle rate or clock rate in the MegaHertz range in the 1980s (“Clock Rate” 2021)! If we look up more about this chip we would learn that it has 4 cores and has hyper-threading, which allows it to effectively perform 8 tasks at once (“What Is Hyper-threading HP® Tech Takes” n.d.). Next we see that there is 16 Gigabytes of memory - this is how much RAM it has and also 2133 MegaHertz (aka 2.133 GHz) of low power double data rate random access memory (LPDDR3), this means that the RAM can process 2,133,000,000 commands every second (Josh Covington 2017; Mukherjee 2019). If you are interested you can checkout more about what this means at this blog post Scott Thornton (2021). However, generally the amount of RAM is more important for assessing performance (Josh Covington 2017; Mukherjee 2019). If we click on the storage button at the top, we can learn about how much storage is available on the computer. If you hover over a section, it tells you what file are accounting for that section of storage that is already being used. 4.2.2 Checking your computer capacity - Windows/PC If you have a PC or Windows computer, the steps may vary depending on your operating system, but try the following: click the “Start” button - which looks like 4 squares together click “Settings” button (gear-shaped) click “System” click “About” See this link for more information. Here we can see that this computer has an Intel(R) Core(TM) i7-4790K CPU @ 4.00 GHz 4.00 GHz chip and 16 Gigabytes of RAM. If we look up this chip we can see that it has 4 cores and 8 threads (due to hyper-threading) allowing for 8 tasks at a time. To find out more information about your storage click the “Storage” button within the “System” tab. Here we can see that this computer has 466 GB + 465 GB = 932 GB across the two drives. The C drive is typically for the operating system, and the D drive is typically where you would install application programs and save files. There are 1000 GB in a TB, thus, this computer has about the same storage as the Mac that we just looked at. 4.3 File Sizes Now let’s think about the files that we might need for our research, how big are files typically for genomic, imaging, and clinical research? Recall this table from earlier about digital data size units: 4.3.1 Genomic data file sizes Genomic data files can be quite large and can require quite a bit of storage and processing power. Here is an image of sizes of some common file types: 4.3.2 Imaging Data File Sizes Imaging data, although often smaller than genomic data, can start to add up quickly with more images and samples. Here is an table of average file sizes for various medical imaging modalities from Liu et al. (2017): [source] Note that depending on the study requirements, several images may be needed for each sample. Thus data storage needs can add up quickly. 4.3.3 Clinical Data File Sizes Really large clinical datasets can also produce sizable file sizes. For example the Healthcare Cost and Utilization Project (HCUP) National (Nationwide) Inpatient Sample (NIS) contains data on more than seven million hospital stays in the United States with regional information. According to the NIS website it “enables analyses of rare conditions, uncommon treatments, and special populations” (“NIS Database Documentation” n.d.). Looking at the file sizes for the NIS data for different states across years, you can see that there are files for some states, such as California as large as 24,000 MB or 2.4 GB (“NIS Database Documentation” n.d.). You can see how this could add up across years and states quite quickly. 4.3.4 Checking file sizes on Mac If you own a Mac and want to check the size of a particular file, look at your file within a finder window. You can open a new finder window by clicking on the button that looks like a square with two colors and a face, typically in the bottom left corner if your dock or the strip of icons on your screen to help you navigate to different application programs. Once you open a finder window, you can navigate to one of your files. If you have the view setting that looks like 4 lines, you will get information about the size of each file. You can right click on a file and click the “Get Info” button. This will give your more specific information. 4.3.5 Checking file sizes on PC/Windows In a similar manner to checking file sizes on a Mac, with a Windows or PC computer, you can navigate to files by first opening the File Explorer application by typing this in the search bar next to the “start” button. Then navigate to a file of interest which will show information about the size in one of the columns to the right, if you hover over the file name, you will get more specific information. 4.4 Computing Options 4.4.1 Personal computers These are computers that your lab might own, such as a laptop, a desktop, used by one individual or maybe just a few individuals in your lab. If you are not performing intensive computational tasks, it is possible that you will only need personal computers for your lab. However, you may find that this changes, and you might require connecting your personal computers to shared computers for more computational power and or storage. 4.4.2 Shared Computing Resources What if you decide that you do need more computational power than your personal computer? You may encounter times where certain informatics tasks take way too long or are not even possible. Evaluating the potential file sizes of the data that you might be working with is a good place to start. However, keep in mind that sometimes certain computations may require more memory than you expect. This is particularly true when working with genomic or image files which are often compressed. So what can you do when you face this issue? One great option, which can be quite affordable is using a server. In terms of hardware, the term server means a computer (often a computer that has much more storage and computing capacity than a typical computer) or groups of computers that can be accessed by other computers using a local network or the internet to perform computations or store data (“Server Definition” n.d.). They are often shared by people, and allow users to perform more intensive computational tasks or store large amounts of data. Read here to learn more (“Server (Computing)” 2021). Using a group of computers is often a much more cost effective option than having one expensive supercomputer (a computer that individually has the computational power of many personal computers) to act as a server (“Supercomputer” 2022). It turns out that buying several less powerful computers is cheaper. In some cases however, an institute or company might even have a sever with multiple supercomputers! As an example use of a server, your lab members could connect to a server from their own computers to allow each of them more computational power. Typically computers that act as servers are set up a bit differently than our personal computers, as they do not need the same functionality and are designed to optimize data storage and computational power. For instance they often don’t have capabilities to support a graphical user interface (meaning the visual display output that you see on your personal computer) (“What Is a Graphical User Interface? Definition and FAQs OmniSci” n.d.). Instead they are typically only accessed by using a command-line interface, meaning that users write code instead of using buttons like they might for a program like Microsoft Word that uses a graphical user interface (“Command-Line Interface” 2022). In order to support this they have memory, processors or CPUs, and storage like your laptop. Here is what a server might look like: In this case we have a group of computers making up this server. Here we see the nodes (the individual computers that make up the server) stacked in columns. Among shared computing resources/servers there are three major options: Clusters - institutional or national resources Grids - institutional or national resources Cloud - commercial or national resources 4.4.3 Computer Cluster In a computing cluster several of the same type of computer (often in close proximity and connected by a local area network with actual cables or an intranet rather than the internet) work together to perform pieces of the same single task simultaneously (“Computer Cluster” 2022). The idea of performing multiple computations simultaneously is called parallel computing (“Parallel Computing” 2021). There are different designs or architectures for clusters. One common one is the Beowulf cluster in which a master computer (called front node or server node) breaks a task up into small pieces that the other computers (called client nodes or simply nodes) perform (“Beowulf Cluster” 2022). For example, if a large file needs to be converted to a different format, pieces of the file will be converted simultaneously by the different nodes. Thus each node is performing the same task just with different pieces of the file. The user has to write code in a special way to specify that they want parallel processing to be used and how. See here for an introduction about how this is done “How to Supercharge Your Bash Workflows with GNU Parallel” (2019). It is important to realize that the CPUs in each of the node computers connected within a cluster are all performing a similar task simultaneously. See here for more information (De Doncker and Hussein, n.d.). 4.4.4 Computer Grid In a computing grid are often different types of computers in different locations work towards an overall common goal by performing different tasks (“What Is Grid Computing? How It Works with Examples” n.d.). Again, just like computer clusters, there are many types of architectures that can be rather simple to very complex. For example you can think of different universities collaborating to perform different computations for the same project. One university might perform computations using gene expression data about a particular population, while another performs computations using data from another population. Importantly each of these universities might use clusters to perform their specific task. Both grids and clusters use a special type of software called middleware to coordinate the various computers involved. Users need to write their scripts in a way that can be performed by multiple computers simultaneously. Users also need to be conscious of how to schedule their tasks and to follow the rules and etiquette of the specific cluster or grid that they are sharing (more on that soon!). See here and herefor more information about the difference between clusters and grids (Lithmee 2018; “Difference Between Grid Computing and Cluster Computing” 2019). 4.4.5 “Cloud” computing More recently, the “Cloud” has become a common computing option. The term “cloud” has become a widely used buzzword (Cha 2015) that actually has a few slightly different definitions that have changed overtime, making it a bit tricky to keep track of. However, the “cloud” is typically meant to describe large computing resources that involve the connection of multiple servers in multiple locations to one another (“Cloud Computing” 2022) using the internet. See here for a deeper description of what the term cloud means today and how it compares to other more traditional shared computing options (“What’s the Difference Between Cloud and Virtualization?” n.d.). Many of us use cloud storage regularly for Google Docs and backing up photos using iPhoto and Google. Cloud computing for research works in a similar way to these systems, in that you can perform computations or store data using an available server that is part of a larger network of servers. This allows for even more computational dependability beyond a more simple cluster or grid. Even if one or multiple servers is down, you can often still use the other servers for the computations that you might need. Furthermore, this also allows for more opportunity to scale your work to a larger extent, as there is generally more computing capacity possible with most cloud resources (“Cloud Computing Vs. Traditional IT Infrastructure Leading Edge” n.d.). Companies like Amazon, Google, Microsoft Azure, and others provide cloud computing resources. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition to these commercial options, there are newer national government funded resource options like Jetstream (described in the next section). We will compare computing options in another chapter coming up. 4.4.6 Accessing Shared Computer Resources It’s important to remember that all of the shared computing options that we previously described involve a data center where are large number of computers are physically housed. You may have access to a HPC (which stands for High Performance Computing) cluster at your institute. This can be a great cost-effective and typically secure option. If your university or institution has a HPC cluster, this means that they have a group of computers acting like a server that people can use to store data or assist with intensive computations. Often institutions can support the cost of many computers within an HPC cluster. This means that multiple computers will simultaneously perform different parts of the computing required for a given task, thus significantly speeding up the process compared to you trying to perform the task on just your computer! If your institute doesn’t have a shared computing resource like the HPCs we just described, you could also consider a national resource option like Xsede. Xsede is led by the University of Illinois National Center for Supercomputing Applications (NCSA) and includes 18 other partnering institutions (which are mostly other universities). Through this partnership, they currently support 16 supercomputers. Universities and non-profit researchers in the United States can request access to their computational and data storage resources. See here for descriptions of the available resources. Here you can see a photo of Stampede2, one of the supercomputers that members of Xsede can utilize. [source] Stampede2, generously funded by the National Science Foundation (NSF) through award ACI-1134872, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin’s flagship supercomputers. See here for more information about how you could possibly connect to and utilize Stampede2. Importantly when you use shared computers like national resources like Stampede2 available through Xsede, as well as institutional HPCs, you will share these resources with many other people and so you need to learn the proper etiquette for using and sharing these resources. We will discuss this more in a coming chapter. However, there is also now an option to access the different XSEDE computing resources through a cloud environment option called Jetstream2. Here is a video about Jetstream2: We will also discuss how the use of these various computing options differ in the next chapters. Importantly there are also some computing platforms that have been especially designed for scientists and specific types of researchers, so it is also useful to know about these options. 4.5 Conclusion We hope that this chapter has given you some more perspective on how large medical research data files can be, as well as given you more familiarity with how well your computer might be able to accommodate the files that you might work with. We also hope that this chapter has provided you with some more awareness about computing options that might be available to you, should you need more capacity than your current computer. In conclusion, here are some of the major take-home messages: A bit is the smallest binary digital data unit. It is a single 0 or 1. A byte is a group of 8 bits, file sizes are typically described using units based on bytes. A typical fancy laptop today might allow for up to 1 TB of storage, however this can quickly get used up if you are working with large data files. Even if you have enough storage for a large file, you might not have enough RAM to actually work with a large data file. Your computer might be too slow to handle that type of work. In which case, you might want to consider using shared computing resources. A server (when describing hardware) is a single computer (typically a supercomputer if just one computer) or group of computers that others can share to help them perform more intensive computational tasks or store large amounts of data. People often connect to these over the internet, but servers can also be connected to by directly using wires in a local network (like in a department to different offices). The computers in a server are optimized for assisting users with computations or storing data. A supercomputer is a computer that has much more storage, memory, and computing capacity than a typical personal computer. Supercomputers are generally much more expensive than using a group of more typical computers that together would have the same collective computing and storage capacity. There are two general types of servers: clusters and grids. Cluster approaches work by having several computers working on pieces of the same task simultaneously in a method called parallel computing. Grid approaches work by having different types of computers working on different tasks. Cloud computing is essentially the use of many servers accessed through the internet. This is often more reliable because there are many servers to use, even if one other users are performing large tasks or if a server goes down. We will talk more about the pros and cons of this option in the coming chapters. If your institute doesn’t provide you access to a shared computing resource and you don’t want to use a commercial cloud option, you could consider options like Xsede and or Jetstream2, which is a national resource that you can request access to. References "],["shared-computing-etiquette.html", "Chapter 5 Shared Computing Etiquette 5.1 General Guidelines for shared computing resources 5.2 Interacting with shared resources 5.3 Running Jobs 5.4 Storage 5.5 Conclusion", " Chapter 5 Shared Computing Etiquette In this chapter we will discuss the proper etiquette for using more traditional shared computing resources, such as an institutional high performance computing cluster server. This will help you to understand what would be required for you to use such resources. Different resources will have slightly different use rules, however, many resources will share common usage requirements. The following is written based on personal experience, the “Rules of Behavior for Computer Network Users Reference Guide” (n.d.) and the “Joint High Performance Computing Exchange (JHPCE) Cluster Orientation” (n.d.). We will use the Johns Hopkins Joint High Performance Computing Exchange (JHPCE) cluster resource as an example to motivate the need for usage rules and proper sharing etiquette for such resources. First let’s learn a bit about this JHPCE. For this particular resource there are about 400 active users.It is optimized for genomic and biomedical research and has 4,000 cores! That’s right, as you can imagine, this is much more powerful than the individual laptops and desktops that researchers at the university have for personal use, which would typically currently only have around 8 cores. There is also 28TB of RAM and 14 PB of storage! Now that you know more about digital sizes, you can appreciate that this server can allow for much faster processing and really large amounts of storage, as again a researchers’ computer might have something like 16 GB of RAM and 1TB of storage. There are 68 nodes that make up the JHPCE currently. As, with most clusters some of the nodes are dedicated to managing users logging in to the cluster and some of the nodes are dedicated to data transferring. Each node has 2-4 CPUs that provide 24-128 cores! As you can see these processors or chips have a lot more cores per each CPU than a typical personal computer. Individual users connect and perform jobs (aka computational tasks) on the cluster using a formal common pool resource (CPR) hierarchy system. What does this mean? This means that it is a shared resource, where if one user overused the resource it would be to the detriment of others and to overcome this there are usage rules and regulations that are enforced by managers of the resource “Common-Pool Resource” (2022). This is important because if a single or a few users used up all the computing resources one day, then the other nearly 400 users would have to delay their work that day, which would not be fair. 5.1 General Guidelines for shared computing resources Each cluster or other shared computing resource will have different rules and requirements, but here are a few general rules to keep in make sure that you don’t accidentally abuse the privilege of sharing an amazing resource like this. Don’t be too worried, most shared resources will give you guidance about their specific rules and will often also have settings that don’t allow users to make major blunders. 5.1.1 Security guidelines One major aspect to consider is keeping the computers in the cluster safe from harm. You wouldn’t want to lose your precious data stored on the cluster and neither would your colleagues! Use a good secure password that is not easy for someone else to guess. Some people suggest using sentences that are easy for you to remember, you could consider a line of lyrics from song or poem that you like, or maybe a movie. Modify part of it to include symbols and numbers (“Guidelines for Strong Passwords · Information Technology Services · Lafayette College” n.d.). Don’t share your password and keep it safe! If you have a Mac, you could consider storing it in your Keychain, alternatively if you have a different type of computer or don’t like the Mac Keychain, consider Dashlane or other password manger services. Luckily both of these options do not come at any extra cost and can be helpful for storing all the passwords we use regularly safely. These are especially good options if your password is difficult for you to remember. Make sure that you abide by any rules regarding storing passwords that might be required by the resource you intend to use. Don’t access a server on a computer that is not authorized to do so. Some servers will require that your computer be authorized for access for added security. It’s a good idea to follow these rules. If you can, perhaps authorize a laptop in case you might need to gain access when you need to be out of town. However if you do so, make sure you also only access such servers with a secure WiFi network. One way to ensure this is is to avoid using public WiFi networks. If you must use a public WiFi network, consider using a virtual private network (VPN) for added security. Here is an article about different VPN options (Gilbertson 2021). Do not alter security settings without authorization. Loosening security settings could pose a risk to the data stored on the server. On the other hand, making more strict security settings could cause other users to not be able to perform their work. Contact the managers of the resource if you think changes need to be made. Immediately report any data security concerns. To protect the integrity of your data and your colleagues, be sure to report anything strange about the shared computing resource to those who manage it so that they can address it right away. Also report to them if you have any security breaches on the computer(s) that you use to access the shared computing resource. 5.1.2 Overall use guidelines Now that we know how to keep the resource safe, let’s next talk about general usage. Don’t install software unless you have permission. It is possible that the software you want to use might already be installed somewhere on the shared computing resource that you are unaware about. In addition, if you install a different version of a software program, it is possible that this version (especially if it is newer) will get automatically called by other people’s scripts. This could actually break their scripts or modify their results. They may have a reason to use an older version of that software, do not assume that they necessarily want the updated version. Instead, let the managers of the resource know. They can inform other users and make sure that everyone’s work will not be disrupted. Don’t use the server for storage or computation that you are not authorized for. This is often a rule for shared computing resources, simply because such shared resources are intended for a specific reason and likely funded for that reason. Such resources are costly, and therefore the computational power should be used only for what it is intended for, otherwise people may view the use of the resources for other purposes as essentially theft. Don’t alter configurations without authorization. This could result unintended and unexpected consequences for other users. 5.1.3 Daily use guidelines Now let’s discuss how you should use such resources on a daily basis. When you submit jobs, make sure you follow the following guidelines. Again consider the fact that there may be more or different requirements for the specific resource that you might be using. Don’t use the login or transfer nodes for your computations. This will cause issues for other users in terms of logging in and transferring their data. This could cause them to be unable to do their work. Think about memory allocation and efficiency. Consider how much RAM and storage is available for people on the shared computing resource. Try not to overload the resource with a really intensive job or jobs that will use most of the resources and either slow down the efficiency of the work for others or not allow them to perform their work at all. This involves: Not using too many nodes if you don’t need to Not using too much RAM on a given node or overall if you don’t need to Not submitting too many jobs at once Communicating with others to give them advanced warning if you are going to submit large or intensive jobs If you have a really large job that you need to perform, talk with the managers of the resource so that you can work out a time when perhaps fewer users would be inconvenienced. Consult the guidelines for your particular resource about how one let’s people know about large jobs before you email the administrators of the resource directly. Often their are communications systems in place for users to let each other know about large jobs. 5.1.4 Communication Guidelines Speaking of communication, let’s dive into that deeper for a bit. Use the proper order for communication. Often shared resources have rules about how they want people to communicate. For example for some resources it is suggested that you first ask your friends and colleagues if you are confused about something, then consult any available forums, if that does not work then directly email the administrators/managers of the resource. Keep in mind that these people are very busy and get lots of communications. Use the ticket system If a resource has a ticket system for users to get support, use it instead of communicating by email. If such a system is in place, then the administrators running it are used to getting requests this way. If you email directly, you may not receive feedback in a timely manner or the email might get lost. 5.1.5 Specific Rules Ultimately it is very important to learn about the rules, practices, and etiquette for the resource that you are using and to follow them. Otherwise, you could lose access. Often other users are a great resource! 5.2 Interacting with shared resources Often you will need to use the command line to interact with a server from your personal computer. To do so on a Mac or a Linux computer you can typically do so using the terminal program that is already on your computer. For PC or Windows computer users, you can use programs like MobaXterm. If you wish to run a program with a graphical interface, then you might need to have a program to help you do so. On Macs, you can download XQuartz. If you use MobaXterm on your PC or Windows computer, then you will already be set. Linux computers also typically should already have what you need. If you are new to Unix commands check out this cheat sheet below. 5.3 Running Jobs Typically a program is used to schedule jobs. Remember that jobs are the individual computational tasks that you ask the server to run. For example, this could be something as simple as moving large files from one directory to another or as complex as running a complicated script on a file. Such job scheduling programs assign jobs to available node resources as they become available and if they have the required resources to meet the job. These programs have their own commands for running jobs, checking resources, and checking jobs. Remember to use the management system to run your jobs using the compute nodes not the login nodes (nodes for users to log in). There are often nodes set up for transferring files as well. In the case of the JHPCE, a program called Sun Grid Engine (SGE) is used, but there are others job management programs. See here for more information on how people use SGE for the JHPCE shared resource. 5.3.1 Specifying memory (RAM) needs Often there is a default file size limit for jobs. For example the JHPCE has a 10GB file size limit for jobs. You may need to specify when you have a job using a file that exceeds the file size limit and set the file size for that job. As you may recall if you are using whole genome files you are likely to exceed the default file limit size. Often you are also given a default amount of RAM for your job as well. Again, you can typically run a job with more RAM if you specify. Similar to the file size limit, you will likely need to set the RAM that you will need for your job if it is above the default limit. Often this involves setting a lower and upper limit to the RAM that your job can use. If your job exceeds that amount of RAM it will be stopped. Typically people call stopping a job “killing” it. The lower and upper limit can be the same number. How do you know how much RAM to assign to your job? Well if you are performing a job with files that are two times the size of the file size default limit, then it might make sense to double the RAM you would typically use. It’s also a good idea to test on one file first if you are going to perform the same job on multiple files. You can then assess how much RAM the job used. First try to perform the job with lower limits and progressively increase until you see that the job was successful and not killed for exceeding the limit. Keep in mind however how much RAM there is on each node. Remember, it is important to not ask for all the RAM on a single node or core on that node, as this will result in you hogging that node and other users will not be able to use RAM on that node or core on that node. Remember that you will likely have the option to use multiple cores, this can also help you to use less RAM across each core. For example, a job that needs 120GB of RAM could use 10 cores with 12 GB of RAM each. Often there will be a limit for the number of jobs, the amount of RAM, and the number of cores that a single user can use beyond the default limits. This is to ensure that a user doesn’t use too many resources causing others to not be able to perform their jobs. Check to see what these limits are and then figure out what the appropriate way is to contact to request for more. Again communication standards and workflows may vary based on the resource. 5.3.2 Checking status It’s also a good idea to check the status of your jobs to see if they worked or got killed. You can check for the expected file outputs or there are commands for the server management software that can help you check currently running jobs. 5.4 Storage Often you will be given a home directory which will likely be backed up, however, other storage directories often will not be. Be careful about where you store your data, as some directories might be for temporary use and get wiped to keep space available for others. 5.5 Conclusion We hope that this chapter has given you some more knowledge about why and how traditional shared computing resources are shared. In conclusion, here are some of the major take-home messages: Shared resources like high performance computing clusters need regulations so that computing resources are shared fairly to allow everyone to get the most work done. Paying attention to security is important to keep everyone’s data and work on the server safe. Although we provided general guidelines, there are likely to be specific guidelines for other resources that you need to adhere to. Often such resources have a communication process to avoid overloading resource administrators/mangers with too many requests. Be sure to follow the appropriate communication etiquette for the resources that you work with. Although there are generally default limits for jobs, users can often consult with the appropriate communication infrastructure to ask to perform larger jobs. References "],["research-platforms.html", "Chapter 6 Research Platforms 6.1 AnVIL 6.2 CyVerse 6.3 SciServer 6.4 Materials Cloud 6.5 Overture 6.6 Globus 6.7 Conclusion", " Chapter 6 Research Platforms In this chapter we will provide examples of computing platforms that are designed to help researchers and that you might find useful for your work. Please note that we aim to provide a general overview of options and thus this is not a complete list. Let us know if there is a platform or system that you think we should include! The major advantage of these platforms is that users can analyze data where it lives, as many platforms host public data. However, some also allow you to upload your own data. There is less need for data transfers, as you can analyze your data, store your data and share it in one place, saving time. Users can sometimes also share how they did their analysis as well, improving reproducibility practices. Additionally, another advantage is that some of these platforms also provide educational material on how to work with data. 6.0.1 Galaxy This section was written by Jeremy Goecks: Galaxy is a web-based computational workbench that connects analysis tools, biomedical datasets, computing resources, a graphical user interface, and a programmatic API. Galaxy (https://galaxyproject.org/) enables accessible, reproducible, and collaborative biomedical data science by anyone regardless of their informatics expertise. There are more than 8,000 analysis tools and 200 visualizations integrated into Galaxy that can be used to process a wide variety of biomedical datasets. This includes tools for analyzing genomic, transcriptomic (RNA-seq), proteomic, metabolomic, microbiome, and imaging datasets, tool suites for single-cell omics and machine learning, and thousands of more tools. Galaxy’s graphical user interface can be used with only a web browser, and there is a programmatic API for performing scripted and automated analyses with Galaxy. Galaxy is used daily by thousands of scientists across the world. A vibrant Galaxy community has deployed hundreds of Galaxy servers across the world, including more than 150 public and three large national/international servers in the United States, Europe, and Australia (https://usegalaxy.org, https://usegalaxy.eu, https://usegalaxy.org.au). The three national/international servers have more than 250,000 registered users who execute &gt;500,000 analysis jobs each month. Galaxy has been cited more than 10,000 times with &gt;20% from papers related to cancer. The Galaxy Tool Shed (https://usegalaxy.org/toolshed) provides a central location where developers can upload tools and visualizations and users can search and install tools and visualizations into any Galaxy server. Galaxy has a large presence in the cancer research community. Galaxy serves as an integration and/or analysis platform for 7 projects in the NCI ITCR program. There is also increasing use of Galaxy in key NIH initiatives such as the NCI Cancer Moonshot Human Tumor Atlas Network (HTAN) and the NHGRI Data Commons, called the AnVIL (https://anvilproject.org/). Galaxy’s user interface, accessible via a web browser, provides access to all Galaxy functionality. The main Galaxy interface has three panels: available tools (left), running analyses and viewing data (middle), and a full history of tools run and datasets generated (right). Datasets for analysis in Galaxy can be uploaded from a laptop or desktop computer or obtained from public data repositories connected to Galaxy. With Galaxy, complex workflows composed of tens or even hundreds of analysis tools can be created and run. In Galaxy’s workflow interface, tools can be added and connected via a simple drag-and-drop approach. Galaxy users can share all their work—analysis histories, workflows, and visualizations—via simple URLs that are available to specific colleagues or a link that anyone can access. Galaxy’s user interface is highly scalable. Tens, hundreds, or even thousands of datasets can be grouped into collections and run in parallel using individual tools or multi-tool workflows. In summary, Galaxy is a popular computational workbench with tools and features for a wide variety of data analyses, and it has broad usage in cancer data analysis. See here for the list of applications supported by Galaxy and here for more information on how to use Galaxy resources. 6.0.2 Terra Terra is a biomedical research computing platform that is based on the Google Cloud platform, that also allows users easier ways to manage the billing of their projects. It provides users with access to data, workflows, interactive analyses using Jupyter Notebooks, RStudio, and Galaxy, data access tools from FireCloud from the Broad Institute (a National Cancer Institute (NCI) Cloud Resource project), as well as workspaces to organize projects and collaborate with others. Terra also has many measures to ensure that data is especially secure and the offer clinical features for ensuring that health data is protected. Note that according users who do upload protected health information must select to use these extra clinical features and enter a formal agree with FireCloud about their data. Importantly users can get access to use Genotype -Tissue Expression (GTEx), Therapeutically Applicable Research to Generate Effective Treatments (TARGET) and The Cancer Genome Atlas (TCGA) data using the platform. See here for information on how. Users can pay for data storage and computing costs for Google Cloud conveniently through Terra. Users can browse data for free. Check out this video for more information: 6.1 AnVIL If you could use some guidance on how to perform analyses using Galaxy and Terra, especially for genomic research, check out AnVIL, the National Human Genome Research Institute (NHGRI) Analysis Visualization and Informatics Lab-space. It also provides access to many important genomic and related datasets from the NHGRI. According to their website: By providing a unified environment for data management and compute, AnVIL eliminates the need for data movement, allows for active threat detection and monitoring, and provides elastic, shared computing resources that can be acquired by researchers as needed. It relies on Terra for the cloud based compute environment, Dockstore for standardized tools and workflows, Gen3 for data management for querying and organizing data, Galaxy tools and environment for analyses with less code requirements, and Bioconductor tools for R programming users. Bioconductor is a project with the mission to catalog, support, and disseminate bioinformatics open-source R packages. Packages have to go through a review process before being included. 6.2 CyVerse CyVerse is a computing platform similar to Galaxy that also offers computing resources for storing, sharing, and working with data with a graphical interface, as well as an API. Computing was previously offered using the cloud computing platform from CyVerse called Atmosphere, which relied on users using virtual machines. Users will now use a new version of Atmosphere with partnership with Jetstream. This allows users to use containers for easier collaboration and also offers US users more computing power and storage. Originally called iPlant Collaborative, it was started by a funding from the National Science Foundation (NSF) to support life sciences research, particularly to support ecology, biodiversity, sustainability, and agriculture research. It is led by the University of Arizona, the Texas Advanced Computing Center, and Cold Spring Harbor Laboratory. It offers access to an environment for performing analyses with Jupyter (for Python mostly) and RStudio (for R mostly) and a variety of tools for Genomic data analysis. See here for a list of applications that are supported by CyVerse. Note that you can also install tools on both platforms. Both CyVerse and Galaxy offer lots of helpful documentation, to help users get started with informatics analyses. See here to learn more. 6.3 SciServer SciServer is also similar to Galaxy in that it is accessible through a web browser and allows users to store, upload, download, share, and work with data and common tools on the same platform. It was originally built for the astrophysics community (and called SkyServer) but it has now been adapted to be used by scientists of all fields and is indeed used by many in the genomics field. It allows users to use Python and R in environments like Jupyter notebooks and RStudio, and also supports (Structured Query Language) SQL for data querying and management and is built on the use of Docker. The main idea of SciServer, is based on this premise: “bring the analysis to the data”. It is free to use after users register. However, users can buy extra resources. Users can keep data private or share their data. As compared to Galaxy, this resources may be better for users with a bit more familiarity with informatics but who require more flexibility, particularly for working with collaborators such as physicists or material scientists as there are more tools supported across disciplines. In addition it also gives users access to very large data sets on Petabyte-scale (note that some of these require special permission to use) and supports developers to create their own web interfaces called SciUIs for particular use cases. For Taghizadeh-Popp et al. (2020) for more information. 6.4 Materials Cloud Another resource that might be of interest to Python users, particular those who collaborate with material scientists, is Materials Cloud. It is designed to promote reproducible work, collaboration, and sharing of resources among scientists, particularly for simulations for the materials science field. Users can share data in a citable way, download data, upload data, share workflows, and perform analyzes. This resource uses AiiDAlab as the computing environment for researchers, which is based on AiiDA. According to their website: AiiDAlab builds on AiiDA as the computational workflow engine, and the Jupyter environment (notebooks, widgets, …) for writing and sharing apps See here to learn more about how AiiDAlab supports the sharing of scientific workflows, particularly for those that use Python. To learn more about Materials Cloud, check out Talirz et al. (2020). 6.5 Overture Overture is a relatively new option for perform large-scale genomic data analyses. You can upload, download, manage, analyze and share your data with authentication and authorization methods to add security. Although designed for genomic research, the data management system can be used for other scientific domains. Currently, additional products are still being developed for analysis, visualization, and sharing. However, several collaborations have created new incredible resources using some of the existing and developing products that might be useful for your research. Alternatively, Overture has options to help you create your own platform, see here for more information. It is compatible with Google, Microsoft Azure and PostgreSQL for storage options. These collaborations using Overture products can be found on the case studies page of the Overture website. For example, the Cancer Genome Collaboratory is one such collaboration. This is A cloud-based resource that allows researchers to perform analyses using International Cancer Genome Consortium (ICGC) cancer genome data, which includes tumor mutation data from the The Cancer Genome Atlas (TCGA) and the Pan-Cancer Analysis of Whole Genomes (PCAWG) mutation data. See here for information about billing, storage capacity, access, and security. In addition, Overture products have also been used to create other data resources, such as the Kids First Data Resource Portal which has childhood cancer and birth defect genomic data for over 76,000 samples, and the National Cancer Institute’s Genomic Data Commons Data portal, which also includes The Cancer Genome Atlas (TCGA) and Therapeutically Applicable Research to Generate Effective Treatments (TARGET). The portal supports some basic [analyses]((https://portal.gdc.cancer.gov/analysis) as well for clinical data statistics and survival analysis. 6.6 Globus Globus provides developers of new platforms to manage, transfer, and share data with special attention to privacy and security. It has been used for several platforms such as the Systems Biology Knowledgebase (KBase), which is focused on integrating data across plants and microbes, Biomedical Research Informatics Network (BIRN), which is a collaborative project to bring biomedical researchers together and share resources and data (Helmer et al. 2011), and Globus Genomics,which uses the Globus data management infrastructure, Amazon web services, and Galaxy workflows to assist researchers with their genomics research endeavors. 6.6.1 BaseSpace Sequence Hub BaseSpace is a platform that allows for data analysis of Illumina sequencing data and syncs easily with any Illumina sequencing machines that you might work with. There are many applications available to help you with your genomics research. They offer a 30 day free trial. 6.6.2 ATLAS.ti ATLAS.ti is designed particularly for qualitative analysis. You can use a variety of data types including video, audio, images, surveys, and social media data. A variety of tools, particularly for text data analysis are provided for methods such as sentiment analysis, which is the process of assigning a general tone or feeling to text and named-entity recognition, which is the process of extracting certain characteristics from texts that are what is called a [named entity] or a real-world object - such as a person’s name or address. Such analyses can be helpful for understanding behaviors that might be associated with cancer risk. Although this type of analysis can be performed using R or Python among other coding languages, ATLAS.ti offers a nice graphical user interface to perform these types of analyses.Furthermore ATLAS.ti offers a great deal of flexibility about such analyses using different data types easily. 6.6.3 GenePattern GenePattern is similar to Galaxy in that it provides a web-based interface for genomic analyses. You can upload your own data, use workflows and pipelines form others and more! See here to access their user guide and here for a quick start guide to using GenePattern Notebook which uses Jupyter Notebooks and GenePattern analysis tools to easily create data analysis reports. Users can also publish and share their notebooks with collaborators or the field, as well as access other people’s notebooks that they can adapt for their own uses. See here for a collection of available notebooks. 6.6.4 XNAT XNAT offers computing resources and tools for performing imaging analysis and for storing and sharing imaging data in a HIPAA complaint manner (more on that in the coming). Developed by the Bukner lab previously at the Washington University and now at Harvard, it supports a variety of imaging data as well as other data types like clinical data. Some tools can be used with a graphical interface and others with the command-line. See here for example use cases. There is also a great deal of documentation available about how to use the tools and resources available at https://wiki.xnat.org/documentation. 6.6.5 OHIF The open health imaging foundation (OHIF) is a web-based imaging analysis platform that is widely used, particularly for radiology analysis, but it also supports whole-slide microscopy image analysis. It was developed by Gordon Harris et al. and can be used for a variety of applications from cardiology to veterinary medicine. Check out these example use cases of OHIF. OHIF also provides thorough documentation with images and videos about how to use the image viewer and tools available. For those interested, Gordon Harris and others are also working on a project called Cornerstone, with the goal of providing software for others to display medical images in web browsers. 6.6.6 PRISM The Platform for Imaging in Precision Medicine called PRISM allows users to work with the vast data available from the Cancer Imaging Archive (TCIA). According to Fred Prior: It is designed to collect, curate and manage Radiology and Pathology images, clinical information associated with those images, annotations and image derived feature sets. PRISM is designed to run on a Kubernettes cluster, but all of the components are containerized so they can run stand-alone or in an alternate orchestration framework. See this article for more information. 6.7 Conclusion We hope that this chapter has given you some more perspective on how the various computing options available designed for researchers like you. We also hope that you may have learned about another platform that can help you to make your research faster and more flexible. In conclusion, here are some of the major take-home messages: Computing platforms are cyberinfastructures that allow you to perform analyses with existing data and or allow you to upload, work with, and share your own data. There are quite a few platforms that are well developed with in depth documentation to support research quite broadly and allow users to work with a diverse set of data. There are also platforms that were designed for specific research needs. References "],["data-management-decisions.html", "Chapter 7 Data Management Decisions 7.1 General Computing Considerations 7.2 Local shared resources vs remote shared resources 7.3 Choosing between remote sharing options 7.4 Overall Decision Process 7.5 Conclusions", " Chapter 7 Data Management Decisions Now that we have discussed a bit about how computers perform computations and described a bit about computing options, lets discuss more about how you might choose the right computing resources for your work. In this chapter we will discuss aspects that you should consider when deciding between different computing resource options. To afford you the best opportunity to perform the informatics research that you would like, it is useful to become familiar with the benefits and drawbacks of various computing options. First we will start out with some general considerations that you should think about when beginning to determine what computing option makes sense for your work. The following are the major decision points for your computing needs: Note: This content was adapted from content by Frederick Tan for the AnVIL project. See his course created with Jeff Leek, Sarah Wheelan, and Kai Kammers here. 7.1 General Computing Considerations Choosing a computing platform depends on several considerations. Asking yourself and your research team the following questions can help you find the right computing platform. Let’s take a bit of a deeper dive now for each of these considerations. 7.1.1 Computation needs Now that you know more about determining your personal computer’s computing and storage capacity, as well as how to determine or estimate the files sizes that you might use for your research, you can begin to assess if your personal computer is up to the task. When determining what your computing needs might be, remember to evaluate how many files you might use in your analyses, the file sizes, the amount of RAM and CPUs (and possibly GPUs that your computer has) and some level of understanding for how intensive the computing tasks are that you plan to perform. How do you assess this? If the files that you intend to use in your analysis are quite large for your computer’s storage capacity, then it is likely that your computer might struggle to work with such files. This might also be the case if you plan to use many many smaller files. Finally, if you plan to perform many steps on your files in your analysis this may also require more computing resources than you have available on your current personal computer. Shared computing options will generally have the capacity to allow you to do your work, unless you have very large data needs and you hope to use a very specialized computing platform that may not support large-scale work. Checking with the local or remote computing options that you are interested in about the computing capacity ahead of time before you start an analysis if you have large data analysis plans would be a good idea. 7.1.2 Data storage Again, now that you know how to assess the data storage potential of your computer, you can decide if your computer can handle storing all the files that you might wish to use in your analysis. Think about your current data analysis plans but keep in mind your future plans as well. If you hope to replicate experiments with more samples, you might run out of storage. One way around this is to by external additional storage (which is also a good idea for backing up your data!). However, if you think that you might have much larger scale research plans in the future, you might want to think about shared computing options. Cloud computing platforms and more traditional servers have different storage capacities, so it is worth checking out the options that might be helpful for your research. Also keep in mind that it will take time to transfer your data, especially if your data is very large. 7.1.3 Multi-institute collaboration If you plan to work with others outside of your institute that would not have access to the same local shared computing resources, then remote computing options would be really helpful for allowing your collaborators to work on the same data together. Cloud platforms especially make it easier for collaboration, as everyone can share the exact same computational environment including hardware, software, and datasets. 7.1.4 Protected data Are you working with protected data that requires special security precautions? [source] If you are working with data that might be protected by HIPAA, such as electronic health records, then special security measures are required to ensure that only authorized users have access to the data. Be careful about both local and remote shared resources. Sometimes extra privacy and data security measures are built in and at other times, you will need perform extra steps to get the data security and privacy that you need. Although many cloud computing systems do not allow for extra privacy, there are platforms that provide compliance with regulations like HIPAA and FedRAMP. 7.1.5 Costs Often local shared computing resources at your institute can be much less expensive than some of the common cloud computing options. However, this is not always the case and if you have very specific analysis goals in mind, the benefit of cloud computing resources, is that you typically only pay for the resources that you actually use. This does also involves learning how costs are calculated for the particular cloud resource, which can be a challenge, but many cloud platforms that were designed for research such as Jetstream or Terra/AnVIL can be very affordable and in some cases some small platforms offer free resources. 7.1.6 Extra guidance Cloud computing platforms such as Galaxy, AnVIL, and GenePattern offer lots of training material and resources about how to actually perform analyses, especially for genomic analyses. Galaxy also supports other types of data, as do many other platforms, as described in the last chapter. Having the extra guidance like that offered with these types of platforms can be very beneficial to investigators that are trying out new methods! 7.1.7 Flexibility If you plan on working with multiple data modalities like for example both imaging and genomic data, consider computing options that are flexible for such analyses. Some cloud computing options designed for research are more general and supportive of this, such as Galaxy to some extent, as well as to a larger extent SciServer, Jetstream, or CyVerse. 7.1.8 Scalability If you hope to start using very large datasets or plan to collaborate with many people using many different data sets, then you might need to keep in mind the future storage and computing capacity of the computing resources that you are considering. Starting with an option that allows for much more data storage and computing such as many of the more general cloud computing options might be best for you in this case, as you might find yourself restricted by smaller cloud platforms or more traditional shared computing resources. 7.1.9 Data access Some cloud computing options already have data available that may be of interest for you and your work. For example Galaxy, AnVIL, and GenePattern provide access to many genomic datasets. Smaller platforms can also have access to data that may be of specific more clinical interest to you as well such as the Cancer Genome Collaboratory, which provides access to data from the International Cancer Genome Consortium (ICGC). 7.1.10 Interface Will you need a graphical interface, a command line interface, or both? What do we mean by this? A graphical user interface or simply just graphical interface or GUI, allows for users to choose functions to perform by interacting with visual representations. They have a “user-centered” design that creates a visual environment where users can for example click on tabs, boxes, or icons for to perform functions. This also often allows users to more directly see plots and other types of visualizations. Galaxy offers a graphical user interface for performing analyses and tasks. For example in the following image we show a GUI for joining two files: Another Graphical User Interface example comes from the OHIF image viewer which has a tool bar for modifying, viewing and annotating images. A command line interface (also known as a character interface) allows for software functions to be performed by specifying through commands written in text. This typically offers more control than a graphical interface, however command line interfaces are often less user friendly as they require that the user know the correct commands to use. For example, one could perform functions in R using Bioconductor packages such as Biostrings with a command line interface: A situation where you might use both a command line interface and a GUI, is using RStudio to perform an analysis in R with Bioconductor packages. RStudio is what is called an IDE or an integrated development environment, which is an application that supports writing code. There are many tools to help you including a console for writing code in R with command line interfacing, as well as graphical interface tools. As shown in this example below, one can inspect and save a plot (that was created with the command line) by using a GUI. The plot was created using the same package as the one used in the command line interface example. Some cloud computing options will have either interface option while others will only have one. This is an important consideration when you decide what computing resources to use. 7.2 Local shared resources vs remote shared resources Often the first decision about cloud computing resources is based on determining if your personal computer can handle your work. If you have already determined that you indeed need more computing power than your personal lab computers, the next decision is between local shared resources (like an institutional server) and remote shared resources, like XSEDE and cloud computing options. See Fischer (2019) for more information about the costs and benefits of using a cloud option like Jetstream for your computational work. We will now discuss some questions that you might ask yourself to make the decision between local and remote shared computing resources. Are local shared computing resources sufficient? When a local computing resource solution already works, one may rightly question the time required to learning how to use a new cloud-based platform. However, when local solutions are insufficient or unsustainable (say other users often use up most of the resources or a server is often down), then the cloud becomes a competitive option. Do you want to work with especially big or controlled access datasets? Increasingly large datasets like the Genotype -Tissue Expression (GTEx), Therapeutically Applicable Research to Generate Effective Treatments (TARGET) or The Cancer Genome Atlas (TCGA) are being stored on the cloud-based platforms. If your work relies on being able to access a large dataset like this, then the cloud may be your only practical option. Do you need to work with collaborators? Computational research increasingly involves larger and larger collaborations. While many systems exist to share work, cloud platforms make it easier for everyone to share the exact same computational environment including hardware, software, and datasets. We will now discuss several opportunities and challenges that cloud computing currently presents. 7.2.1 Benefits of Cloud Computing The state of Cloud computing is continually evolving. Here, we highlight the main current benefits: Sharing history The first major benefit is the increasing ease with which one can share and collaborate on research projects. Shown here is the History feature of Galaxy. Using this, one can share not only what datasets they used but also every computational manipulation that was performed. By sharing such a history, one can reproduce an analysis in its entirety (if they use the same data), allowing collaborators to offer comments and extend upon the work with ease. Sharing Workflows between Platforms While sharing complete analysis histories is for the most part constrained to a particular software platform, a second benefit that has arisen is the ability to share workflows between platforms.Dockstore is a great open-source and free option to share and find bioinformatics workflows that can be launched using different platforms. Shown here is a diagram of an analysis pipeline to create a custom reference for single cell 10x data using cell ranger published by the Klarman Cell Observatory on Dockstore. Users can launch the workflow on various supported platforms such as Terra or AnVIL.: This higher level abstraction coupled with container technology allows this multistep analysis to be run with relative ease on supporting platforms like Terra, AnVIL, or DNAnexus, which is yet another computing platform company, although with generally more costs associated as compared to Terra or AnVIL. Using Commodity Hardware The third Benefit we highlight is the increasing ease by which one can provision commodity hardware at scale. What this means is that you can pay reasonable costs to complete your analysis in less time by renting hundreds to tens of thousands of Cloud-based computers – importantly stopping the bill when your analysis is complete. Specialized hardware like GPUs and large memory nodes are also available for rent allowing you to pay only for what you need. Less Etiquette Relative to more traditional shared computing resources, you don’t need to worry as much about sharing etiquette with cloud computing options because these resources typically have more than enough to go around. There is a trade off in that you will need to learn how to work with the cloud computing platform, however you can be more independent about what software you use and how many resources you use without bothering others. Data Access Easy access to large datasets that might be otherwise difficult to work with. Now many cloud computing resources, especially those designed for research, also allow users access to relevant datasets. 7.2.2 Challenges of Cloud Computing Balancing these benefits are the following challenges: Data Transfer Data transfer and data management remains a cumbersome task. While storing data in the cloud has its advantages, it also has corresponding storage costs. Thus, careful planning is necessary with regards to what data will be stored where, as well as budgeting the time necessary to transfer data back and forth. However if you have yet to start work on a local shared computing resource than the work to transfer may be comparable. Additionally if you plan to use public data that is accessible through a platform designed for research than you may in fact have less data transfer needs when working with a cloud computing option. Data Security Most cloud resources offer features that make it easier to access and share data, and these features often come at the expense of security. Thus, special precautions must be implemented to securely store protected datasets such as human genome sequences and electronic health records. Some of the specialized research platforms allow for this as described in the previous chapter. Make sure you check what is required to set up an extra security protection - often this will not be automatic. Also keep in mind that although local shared resources like that of your university often have good security and data privacy policies and protection mechanisms, this is not always the case. Costs Controlling costs, especially with regards to storage, presents a third formidable challenge. As many cloud providers naturally want to encourage usage of their platforms, users must be aware of how much money is currently being spent and be able to project how much money is likely to be spent in the future. While software platforms can help mitigate these challenges, cloud computing still incurs costs from the underlying hardware providers. It can be a challenge to understand how this is billed. IT A final challenge is that many IT support staff do not have extensive experience managing cloud resources. Should IT choose to support analysis on the cloud, they would face the aforementioned challenges of understanding and supporting data management, security compliance, and cost management. Fortunately, large initiatives like AnVIL, Galaxy, and CyVerse continue to work on democratizing access to cloud computing by tackling many of these challenges. 7.3 Choosing between remote sharing options The final major decision, should you decide that you want to go with a remote sharing option is to decide which remote computing resource to go with. This decision should be based on the following: 7.4 Overall Decision Process We suggest evaluating your computing needs based on the following decision tree. 7.5 Conclusions We hope that this chapter has given you some more perspective on how to make important computing decisions to make the most out of your work. In conclusion, here are some of the major take-home messages: The three major computing decisions are: 1. Personal computer vs Shared resource, 2. Local shared resource vs. Remote Shared resource, 3. Which shared resource. Start first with determining if your personal computer can handle your work or if you have plans to collaborate with others at different institutes. The following are general considerations that can help you make each of the 3 decisions: 1. How computationally intensive will the work be? 2. How much data storage is and will be needed? 3. Do I plan on collaborating with others outside my institute? 4. Does my data that need extra privacy protection? 5. How much money can I spend on computing? 6. Do I want extra guidance for my informatics work? 7. Do I need flexibility? Might I work with more data modalities in the future? 8. Do I need scalability? Will I soon work with more data? 9. Do I need access to large controlled datasets? 10. What kind of interface would be helpful? The main benefits of cloud options are easier sharing of data, code, and workflows even across different computing platforms, access to large amounts of computing resources, generally only need to pay for the resource you actually use, less need to be as conscious of proper etiquette for sharing computing resources. The main drawbacks of cloud options in general are: 1. if you were already using a shared resource then migrating to the cloud will require data transfer effort and time, 2. Some cloud computing options do not provide data privacy protection that may be needed for certain types of data, 3. costs calculations can be especially confusing and you will need to learn how it works for the particular resource that you are interested in using, 4. There will be less IT support form your local IT department as many of these resources have their own infrastructure, however many options provide their own guidance and support. More and more cloud computing options are being developed and designed specifically for research, consider the following questions when choosing between cloud-based options: 1. Does the resource offer the privacy protection needed? 2. Does the resource allow for me to use the data modalities I need? 3. Does the resource already have data that I might want? 4. Does the resource provide extra guidance that might be helpful for my work? 5. Does the resource offer the interface that I would like to work with? 6. Does the resource offer me access to a relevant community for my work that would be helpful? References "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     In memory of James Taylor, who was instrumental in initiating this project.   Credits Names Pedagogy Lead Content Instructor Carrie Wright Content Editors/Reviewers Sarah Wheelan Content Directors Jeff Leek, Sarah Wheelan Content Consultants (Shared computing etiquette) Brian Caffo, Mark Miller Content Consultants (Basic building block of computers) Adam Wright, Stephen Edwards Content Consultants (Binary data to computations) Adam Wright, Stephen Edwards Acknowledgments Garvi Sheth - thank you for your feedback Production Content Publisher Ira Gooding Content Publishing Reviewers Ira Gooding, Candace Savonen Technical Course Publishing Engineer Carrie Wright Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (ottr) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator Carrie Wright Funding Funder National Cancer Institute (NCI) UE5 CA254170 Funding Staff Emily Voeglein, Fallon Bachman   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.3 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2022-03-04 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## bookdown 0.24 2022-02-15 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.6.1 2022-01-22 [1] CRAN (R 4.0.2) ## hms 0.5.3 2020-01-08 [1] RSPM (R 4.0.0) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.0.2) ## knitr 1.33 2022-02-15 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 2.0.2 2022-01-26 [1] CRAN (R 4.0.2) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## ottrpal 0.1.2 2022-02-15 [1] Github (jhudsl/ottrpal@1018848) ## pillar 1.4.6 2020-07-10 [1] RSPM (R 4.0.2) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] RSPM (R 4.0.3) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## readr 1.4.0 2020-10-05 [1] RSPM (R 4.0.2) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2022-02-15 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2022-02-15 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2022-02-15 [1] Github (R-lib/testthat@e99155a) ## tibble 3.0.3 2020-07-10 [1] RSPM (R 4.0.2) ## usethis 2.1.5.9000 2022-02-15 [1] Github (r-lib/usethis@57b109a) ## vctrs 0.3.4 2020-08-29 [1] RSPM (R 4.0.2) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2022-02-15 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
